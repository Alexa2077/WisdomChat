



<a name="yumpY"></a>
# 1 为什么大模型需要微调？

预训练模型（Pre-trained Model），或者说基础模型（Foundation Model），已经可以完成很多任务，比如回答问题、总结数据、编写代码等。但是，并没有一个模型可以解决所有的问题，尤其是行业内的专业问答、关于某个组织自身的信息等，是通用大模型所无法触及的。在这种情况下，就需要使用特定的数据集，对合适的基础模型进行微调，以完成特定的任务、回答特定的问题等。在这种情况下，微调就成了重要的手段。

**大模型微调（Fine-tuning）**是指在已经预训练好的大语言模型基础上，使用特定的数据集进行进一步的训练，使模型适应特定任务或领域。


<a name="PZ7dp"></a>
# 2 微调方法：
<a name="OWX9R"></a>
## 2.1 方法分型：
<a name="HLRTk"></a>
### 1 从微调的层级划分：
根据微调对整个预训练模型的调整程度，微调可以分为**全微调**和**部分微调两**个方法：

- 全微调（Full Fine-tuning）：全微调是指对整个预训练模型进行微调，包括所有模型参数。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。这种微调方法通常适用于任务和预训练模型之间存在较大差异，或任务需要模型具有高度灵活性和自适应能力的情况。全微调需要较多计算资源和时间，但可以获得更好的性能。
- 但FFT也会带来一些问题，影响比较大的问题，主要有以下两个：
   - 一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；
   - 一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。

- 部分微调（Repurposing）：部分微调是指在微调过程中**只更新模型的顶层或少数几层**，而保持预训练模型的底层参数不变。这种方法的目的是在保留预训练模型的通用知识的同时，通过微调顶层来适应特定任务。Repurposing通常适用于目标任务与预训练模型之间有**一定相似性**的情况，或者任务数据集较小的情况。由于只更新少数层，Repurposing相对于Full Fine-tuning需要较少的计算资源和时间，但在某些情况下性能可能会有所降低。
- 比如：PEFT(Parameter-Efficient Fine Tuning)，PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。

<a name="uTOim"></a>
### 2 从训练数据进行划分：

- 一个是**监督式微调SFT(Supervised Fine Tuning)**，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调；

- 一个是**基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback)，**这个方案的主要特点是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；

- 还有一个是**基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback)**，这个原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。

不同的分类角度，只是侧重点不一样，对同一个大模型的微调，也不局限于某一个方案，可以多个方案一起。<br />微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。


<a name="zs8Nu"></a>
## 2.2 按照微调的层级和范围划分：
也可划分成：<br />fine-tuning、parameter-efficient fine-tuning和prompt-tuning


PEFT（Parameter-Efficient Fine-Tuning）是hugging face开源的一个参数高效微调大模型的工具，里面集成了4种微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果，使得在GPU资源不足的情况下也可以微调大模型。<br />![](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716285778242-96e11be4-fba7-4705-8b74-22f4f9f51f49.png#averageHue=%23f1de94&clientId=u0cbaf271-9e00-4&from=paste&id=ub915f047&originHeight=529&originWidth=1080&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=u23e568f4-1434-4bbc-8ad9-ed49e30f83b&title=)

<a name="cmUPv"></a>
### 1 Adapter-based Methods（基于适配器的方法）:
《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。

于是他们设计了如下图所示的 Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716280997204-7786ac54-be9f-4bd3-9378-8e1aa94002cd.png#averageHue=%23f5f5f0&clientId=u0cbaf271-9e00-4&from=paste&height=304&id=ub10998aa&originHeight=543&originWidth=717&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=142894&status=done&style=none&taskId=uba148e39-7476-4b65-acd2-f3f7b2f49e0&title=&width=401.6000061035156)

这种方法节省了资源，因为它不需要对整个模型进行微调。示例有AdapterDrop、Parallel Adapter、Residual Adapter等。

<a name="c3yCY"></a>
### 2 Prompt-based Methods（基于提示的方法）

这个分支侧重于使用连续的提示（如嵌入向量）来调整模型的行为，而不是直接修改模型的权重。这类方法通常用于生成任务，例如文本生成。提示可以视为模型输入的一部分，它们会被训练以激发模型生成特定的输出。示例包括Prefix-tuning、Prompt tuning等.
<a name="UrpU0"></a>
### 3 Low-rank Adaptation（低秩适配）:

低秩适配方法致力于将模型权重的改变限制在一个低秩子空间内。这通常涉及对模型的权重矩阵进行分解，只微调其中的一小部分参数。这样可以有效减少计算资源的消耗，同时仍然允许模型有足够的灵活性来学习新任务。LoRA和它的变种，如Q-LoRA、Delta-LoRA、LoRA-FA等，都属于这个类别。

<a name="ieNxQ"></a>
### 4. Sparse Methods（稀疏方法）:
这个分支包括那些仅更新模型中一小部分参数的方法。这些参数被选为最有可能影响到任务性能的，而其他参数则保持不变。稀疏方法的优点在于它们通常能够更高效地利用资源。例如有Intrinsic SAID、Fish Mask、BitFit等。

<a name="miu8x"></a>
### 5. Others（其他方法）:
这一分支可能包括不易归类到上述任何一类的其他方法，或者是结合了多种技术的混合方法。这些方法可能包括特定的结构改变、算法优化等，用以提高微调过程的效率或者效果。

<a name="TZ9jh"></a>
## 2.3 微调方法详细介绍：
<a name="LWoqT"></a>
### 1 Fine tuning
**经典的Fine tuning就是 算是叫做继续训练？**<br />经典的Fine tuning方法包括将预训练模型与少量特定任务数据一起**继续训练**。在这个过程中，预训练模型的权重被更新，以更好地适应任务。所需的Fine-tuning量取决于预训练语料库和任务特定语料库之间的相似性。如果两者相似，可能只需要少量的Fine tuning。如果两者不相似，则可能需要更多的Fine tuning。<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716280128125-f321f27b-1ee0-4eec-b565-d81af00b1375.png#averageHue=%23f3eeed&clientId=u0cbaf271-9e00-4&from=paste&height=242&id=uce5394a2&originHeight=236&originWidth=681&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=69259&status=done&style=none&taskId=ub65b2315-4ff8-4603-a677-529cbe88db8&title=&width=697.7999877929688)

<a name="f9A9x"></a>
### 2 Prompt Tuning（P-tuning）
Prompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法。参数高效性微调方法中实现最简单的方法还是Prompt tuning(也就是我们常说的P-Tuning)，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型。

![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716280337162-81be2b19-65f4-494a-9352-212465662d32.png#averageHue=%23f2f0ef&clientId=u0cbaf271-9e00-4&from=paste&id=ub2011158&originHeight=1322&originWidth=5727&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=1624501&status=done&style=none&taskId=u5fad8c7d-cd86-4399-92fb-ac2da66d171&title=)

经典的Prompt tuning方式不涉及对底层模型的任何参数更新。相反，它侧重于精心制作可以指导预训练模型生成所需输出的输入提示或模板。主要结构是利用了一个prompt encoder（BiLSTM+MLP），将一些pseudo prompt先encode（离散token）再与input embedding进行拼接，同时利用LSTM进行 Reparamerization 加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果。然后结合（capital，Britain）生成得到结果，再优化生成的encoder部分。

![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716284438608-540e7afa-443b-4ec5-8ec5-bfdfe93de6bf.png#averageHue=%23dbdad8&clientId=u0cbaf271-9e00-4&from=paste&id=u482fdf8e&originHeight=900&originWidth=1600&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=305844&status=done&style=none&taskId=uc78cc723-ff97-4be9-ad93-f7889b631d7&title=)

但是P-tuning v1有两个显著缺点：任务不通用和规模不通用。在一些复杂的自然语言理解NLU任务上效果很差，同时预训练模型的参数量不能过小。具体的效果论文中提到以下几点：

- Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为1也能达到不错的效果，Prompt 长度为20就能达到极好效果。
- Prompt初始化方式影响：Random Uniform 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。
- 预训练的方式：LM Adaptation 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。
- 微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，zero shot 也能取得不错效果。
- 当参数达到100亿规模与全参数微调方式效果无异。


<a name="LIx3l"></a>
### 3 Prefix Tuning
2021年论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法。与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。

Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716284484631-1a27b1b3-1159-4dfb-9f52-13dad694cc0c.png#averageHue=%23dfdede&clientId=u0cbaf271-9e00-4&from=paste&id=u024eea29&originHeight=900&originWidth=1600&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=272179&status=done&style=none&taskId=u064a4625-2b9d-472a-bf33-51a665a85bb&title=)<br />prefix-tuning技术，相对于fine-tuning，在调节模型的过程中只优化一小段可学习的continuous task-specific vector（prefix）而不是整个模型的参数。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而Prefix 则是可以学习的“隐式”的提示。手动尝试最优的提示无异于大海捞针，于是便有了自动离散提示搜索的方法，但提示是离散的，神经网络是连续的，所以寻找的最优提示可能是次优的。<br />![](https://cdn.nlark.com/yuque/0/2024/webp/38497976/1716280501261-d12c6c6d-129e-4d32-a3fe-9a714035becd.webp#averageHue=%23f3f3f0&clientId=u0cbaf271-9e00-4&from=paste&height=284&id=u1da80734&originHeight=313&originWidth=720&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=u2dc4f756-bbe6-4816-9fae-b69fd03b4eb&title=&width=653)

<a name="PKjJu"></a>
### 4  P-tuning v2
V2版本主要是基于P-tuning和prefix-tuning技术，引入Deep Prompt Encoding和Multi-task Learning等策略进行优化的。实验表明，仅精调0.1%参数量，在330M到10B不同参数规模LM模型上，均取得和Fine-tuning相比肩的性能。

论文《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》从标题就可以看出，P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果。也就是说当前 Prompt Tuning 方法在这两个方面都存在局限性。

不同模型规模：Prompt Tuning 和 P-tuning 这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning 类似的效果，而参数规模较小时效果则很差。<br />不同任务类型：Prompt Tuning 和 P-tuning 这两种方法在 sequence tagging 任务上表现都很差。


<a name="wszGF"></a>
### 5  LoRA系列：

LoRA：<br />LoRA是跟Prompt Tuning和Prefix Tuning完全不相同的另一条技术路线。<br />LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716284516474-5039e772-1b07-4f1c-a016-ab97179c7f49.png#averageHue=%23d7d5d3&clientId=u0cbaf271-9e00-4&from=paste&id=u0e04b036&originHeight=900&originWidth=1600&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=218114&status=done&style=none&taskId=u15a3626d-d87a-4434-9955-4a3a788e599&title=)<br />通俗讲人话：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。

- 首先, 要适配特定的下游任务，要训练一个特定的模型，将Y=WX变成Y=(W+∆W)X，这里面∆W主是我们要微调得到的结果；
- 其次，将∆W进行低维分解∆W=AB (∆W为m * n维，A为m * r维，B为r * n维，r就是上述假设中的低维)；
- 接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。
- 另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W`。


QLoRA：<br />LoRA 效果已经非常好了，可以媲美全量微调的效果了，那为什么还要有个QLoRA呢？<br />这里先简单介绍一下，量化（Quantization）。<br />量化，是一种在保证模型效果基本不降低的前提下，通过降低参数的精度，来减少模型对于计算资源的需求的方法。<br />量化的核心目标**是降成本**，降训练成本，特别是降后期的推理成本。<br />QLoRA就是量化版的LoRA，它是在LoRA的基础上，进行了进一步的量化，将原本用16bit表示的参数，降为用4bit来表示，可以在保证模型效果的同时，极大地降低成本。<br />论文中举的例子，65B的LLaMA 的微调要780GB的GPU内存；而用了QLoRA之后，只需要48GB。效果相当惊人！


AdaLoRA:<br />预训练语言模型中的不同权重参数对下游任务的贡献是不同的。因此需要更加智能地分配参数预算，以便在微调过程中更加高效地更新那些对模型性能贡献较大的参数。<br />具体来说，通过奇异值分解将权重矩阵分解为增量矩阵，并根据新的重要性度量动态地调整每个增量矩阵中奇异值的大小。这样可以使得在微调过程中只更新那些对模型性能贡献较大或必要的参数，从而提高了模型性能和参数效率。



<a name="ouwwd"></a>
## 2.4 Prompt-Tuning、Instruction-Tuning和Chain-of-Thought区别
参考：[https://www.datalearner.com/blog/1051681306547159](https://www.datalearner.com/blog/1051681306547159)

![](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716284995992-885834e3-89a5-4b7d-a534-a09568e10908.png#averageHue=%237ca768&clientId=u0cbaf271-9e00-4&from=paste&id=ua5b77058&originHeight=386&originWidth=974&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=ufc5faae1-d317-4595-9353-14f4be213c9&title=)

上面已经介绍了prompt：
<a name="AB1us"></a>
### 1 Instruction-Tuning介绍

与Prompt不同，Instruction通常是一种更详细的文本，用于指导模型执行特定操作或完成任务。Instruction可以是计算机程序或脚本，也可以是人类编写的指导性文本。Instruction的目的是告诉模型如何处理数据或执行某个操作，而不是简单地提供上下文或任务相关信息。

因此，Prompt和Instruction都是用于指导模型生成输出的文本，但它们的目的和使用方式是不同的。Prompt更多地用于帮助模型理解任务和上下文，而Instruction则更多地用于指导模型执行具体操作或完成任务。

instruction-Tuning这种方法Google和OpenAI几乎都同时使用了。<br />Google Research在2021年的论文《Finetuned Language Models Are Zero-Shot Learners》中提出了instruction-tuning。Google认为instruction-tuning是一种简单的方法来提高语言模型的zero-shot学习能力。<br />![](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716285099760-57502d4f-c51e-4153-bafb-b5d2d7533a54.png#averageHue=%23f2e9e2&clientId=u0cbaf271-9e00-4&from=paste&id=ua6df494f&originHeight=615&originWidth=869&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=u15c2ed0a-035e-4724-8dc5-cb5b0e7ab14&title=)<br />而OpenAI在InstructGPT中也是类似的想法！InstructGPT就是ChatGPT的前身！<br />Instruction可以提供更直接的任务指导，以帮助模型执行任务。对于问答任务，Instruction可以提供具体的指令，例如“请回答下列问题：谁是美国第一位总统？”，并将文本段落作为输入提供给模型。

以InstructGPT为例，其基本流程如下：

- 准备自然语言指令集：针对特定任务，准备一组自然语言指令，描述任务类型和任务目标，例如情感分类任务的指令可以是“该文本的情感是正面的还是负面的？”。
- 准备训练数据集：针对特定任务，准备一个标记化的数据集，其中每个数据样本都包含输入文本和标签，例如情感分类任务的标签可以是“正面”或“负面”。
- 将自然语言指令和数据集转换为模型输入：将自然语言指令和数据集转换为模型输入，例如对于情感分类任务，将自然语言指令和文本拼接作为输入，例如：“该文本的情感是正面的还是负面的？这家餐厅的食物很好吃。”
- 在指令上进行微调：在指令上进行微调，以适应特定任务的需求，提高模型在任务上的性能。

这样的方式训练了出来的模型可以让模型更好地识别输入的意图，同时也在zero-shot中表现更好！<br />而Instruction-tuning的主要思想就是在输入阶段对指令做调整以其适合更好地模型输出！（本文最后一段是一个GPT-4给出的使用Instruction-tuning微调GPT-4的方法）


<a name="jK0PK"></a>
### 2 Chain-of-Thought
Chain-of-thought 是一种处理复杂问题或执行多步骤任务的技巧，通常用于大型预训练语言模型中。这种方法允许模型在多个步骤中生成连贯的回答，从而更好地解决问题或完成任务。<br />在 chain-of-thought 方法中，模型的输出被视为一个序列，每个部分都是一个独立的“思考链”或步骤。模型通过将先前的输出作为后续输入的一部分来迭代地生成这些部分，这样可以让模型在一定程度上模拟人类解决问题的过程。<br />以下是一个简单的示例，说明语言模型如何解决一个数学问题：<br />问题：计算 3 * (4 + 5)<br />使用 chain-of-thought，我们可以将问题分解成以下步骤：

- 将问题输入模型：“计算 3 * (4 + 5)”
- 模型输出第一步的结果：“计算 4 + 5”
- 将上一步的结果作为输入，再次输入模型：“计算 3 * 9”
- 模型输出最终结果：“结果是 27”

在这个例子中，我们可以看到模型是如何通过一系列连贯的思考链（步骤）来逐步解决问题的。那么这样的能力是如何获得的？这就是chain-of-thought技术。通过思维链式方法训练大型语言模型需要将训练过程分解成较小、相互关联的任务，以帮助模型理解和生成连贯、上下文感知的响应。

就是让大模型一步一步的来完成任务。

<a name="UR2Rh"></a>
### 3 总结：
我的感觉就是，他们输入的从少到多； prompt 输入的最少，instruction是直接说干啥干啥，目标明确；COT是变成一步一步分析，最终得到结果，一次性的输入很多次的感觉。

Prompt-tuning、instruction-tuning和chain-of-thought都是用于训练大型语言模型的方法，它们都有助于提高模型的生成能力和上下文理解能力，但是它们的方法和目的略有不同。

- Prompt-tuning：Prompt-tuning是一种使用自然语言提示（prompt）的方法，以指导模型生成特定的输出。这种方法的目的是通过对模型进行定向训练，使其在特定任务上表现出更好的性能。与其他方法不同，Prompt-tuning的重点在于设计良好的提示，这些提示可以引导模型生成准确、上下文相关的响应。
- Instruction-tuning：Instruction-tuning是一种通过为模型提供任务相关的指令来指导模型学习的方法。这种方法的目的是使模型更好地理解任务的要求，并提高其生成能力和上下文理解能力。Instruction-tuning通常需要较少的训练数据，并且可以提高模型的泛化性能。
- Chain-of-thought：Chain-of-thought是一种通过分解训练过程为较小的相互关联的任务来训练模型的方法。这种方法的目的是使模型能够理解和维护文本中的思维链，从而生成连贯的、上下文相关的响应。与其他方法不同，Chain-of-thought的重点在于将训练过程分解为一系列逐步更复杂的任务，并使用注意机制来帮助模型集中于相关的部分。

总之，这些方法都有助于提高大型语言模型的生成能力和上下文理解能力，但是它们的方法和目的略有不同。Prompt-tuning和instruction-tuning通常用于特定任务的优化，而Chain-of-thought通常用于提高模型的生成能力和上下文理解能力。



<a name="OOjOg"></a>
# 3 大模型微调步骤总结：

大模型微调如上文所述有很多方法，并且对于每种方法都会有不同的微调流程、方式、准备工作和周期。然而大部分的大模型微调，都有以下几个主要步骤，并需要做相关的准备：<br />![](https://cdn.nlark.com/yuque/0/2024/png/38497976/1716282200984-919d317d-6843-473c-82e6-26bfe928578b.png#averageHue=%23fbf9f4&clientId=u0cbaf271-9e00-4&from=paste&id=u8fc9aece&originHeight=339&originWidth=778&originalType=url&ratio=1.25&rotation=0&showTitle=false&status=done&style=none&taskId=udf787fe4-168e-4e2c-9eac-1f85e1c4fed&title=)

1. 准备数据集：

收集和准备与目标任务相关的训练数据集。确保数据集质量和标注准确性，并进行必要的数据清洗和预处理。<br />对数据进行预处理，包括清洗、分词、编码等。

2. 选择预训练模型/基础模型：

根据目标任务的性质和数据集的特点，选择适合的预训练模型。如BERT、GPT-3等。

3. 设定微调策略：

根据任务需求和可用资源，选择适当的微调策略。考虑是进行全微调还是部分微调，以及微调的层级和范围。

4. 设置超参数：

确定微调过程中的超参数，如学习率、批量大小、训练轮数等。这些超参数的选择对微调的性能和收敛速度有重要影响。

5. 初始化模型参数：

根据预训练模型的权重，初始化微调模型的参数。对于全微调，所有模型参数都会被随机初始化；对于部分微调，只有顶层或少数层的参数会被随机初始化。

6. 进行微调训练：

使用准备好的数据集和微调策略，对模型进行训练。在训练过程中，根据设定的超参数和优化算法，逐渐调整模型参数以最小化损失函数。

7. 模型评估和调优：

在训练过程中，使用验证集对模型进行定期评估，并根据评估结果调整超参数或微调策略。这有助于提高模型的性能和泛化能力。

8. 测试模型性能：

在微调完成后，使用测试集对最终的微调模型进行评估，以获得最终的性能指标。这有助于评估模型在实际应用中的表现。

9. 模型部署和应用：

将微调完成的模型部署到实际应用中，并进行进一步的优化和调整，以满足实际需求。




<a name="g993x"></a>
# 参考：

1，[https://zhuanlan.zhihu.com/p/673789772](https://zhuanlan.zhihu.com/p/673789772)

2，[https://zhuanlan.zhihu.com/p/650287173](https://zhuanlan.zhihu.com/p/650287173)

3，[https://zhuanlan.zhihu.com/p/647843722](https://zhuanlan.zhihu.com/p/647843722)

[https://www.datalearner.com/blog/1051681306547159](https://www.datalearner.com/blog/1051681306547159)





